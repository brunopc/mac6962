

\section{Aula 12 de Agosto de 2019}
\label{2019_08_12}
\subsection{Elementos distintos em um stream}
\paragraph{} Uma sequência $\sigma = (s_1, s_2, ..., s_n)$, $s_i \in U = [n] = {1, ..., n}$ que deve ser olhada uma única vez ou não pode ser guardada. Queremos saber quantos valores distintos ocorrem em $\sigma$. Seja $\Delta$ o número de valores distintos em $\sigma$ e $x_k$ o número de ocorrências de $k \in [n]$ em $\sigma$, queremos saber para quantos $k$ temos $x_k>0$.
$$X = (x_1, ..., x_n)$$
\paragraph{} Queremos resolver esse problema com memória polilogarítmica $\mathcal{O}((\log{}n)^{\mathcal{O}(1)})$. Usamos aleatorização e estimamos um $\Delta$ menor que um fator multiplicativo.
\paragraph{}Primeiramente, um problema mais simples: Queremos um algoritmo $\mathcal{A}$ tal que dado T temos:
\begin{enumerate}
    \centering
    \item Se $\Delta \geq (1+\epsilon)T$, então $\mathcal{P}(\mathcal{A}(\sigma) = 1) \geq 1 - \delta$
    \item Se $\Delta \leq (1-\epsilon)T$, então $\mathcal{P}(\mathcal{A}(\sigma) = 0) \geq 1 - \delta$
\end{enumerate}

\subsection{Algoritmo $\mathcal{A}$'}
\paragraph{} Escolhe $\mathcal{S} \subseteq [n]$ com $\mathcal{P}( i \in \mathcal{S}) = \frac{1}{T},\ \forall i \in [n]$ eventos independestes. Interpretando o elemento $s_j$ de $\sigma$ como:
$$x_k \leftarrow x_k + 1$$
\paragraph{}Em que $k=s_j$. Começamos com $X$ vazio. O algoritmo $\mathcal{A}$' mantém um "esboço" de $X$, a saber:
$$\sigma(S) = \sum_{i \in \mathcal{S}}x_i$$
\paragraph{} O algoritmo $\mathcal{A}$' devolve:
\begin{enumerate}
    \centering
    \item $\mathcal{A}'(\sigma) = 1$, se $\sigma(\mathcal{S}) > 0$
    \item $\mathcal{A}'(\sigma) = 0$, se $\sigma(\mathcal{S}) = 0$
\end{enumerate}
\paragraph{} Temos que:
$$\mathcal{P}_0 = \mathcal{P}(\sigma(\mathcal{S})= 0) = (1-\frac{1}{T})^\Delta$$
\paragraph{} Se $\Delta \geq (1+\epsilon)T$, então:
$$\mathcal{P}_0 \leq e ^{-\frac{\Delta}{T}} \leq e ^{-(1+\epsilon)} \leq \frac{1}{e}(1-\frac{\epsilon}{2})$$
\paragraph{} para $(\epsilon \leq \epsilon_0)$
\paragraph{} Se $\Delta \leq (1-\epsilon)T$, então:
$$\mathcal{P}_0 \geq exp(-\frac{1}{T}-\frac{1}{T^2})^\Delta \geq e ^{-(1+\epsilon)-T^{-1}} = \frac{1}{e}\frac{e^\epsilon}{e^{T^{-1}}}$$
$$\frac{1}{e}\frac{e^\epsilon}{e^{T^{-1}}} \geq \frac{1}{e}\frac{e^\epsilon}{e^\frac{\epsilon}{2}}=\frac{1}{e}e^\frac{\epsilon}{2} \geq \frac{1}{e}(1+\frac{\epsilon}{2})$$
\paragraph{} para $T \geq \frac{2}{\epsilon}$

\subsection{Algoritmo $\mathcal{A}$}
\paragraph{} Tomando:
$$k = \frac{2e^2}{\epsilon^2}\log{\frac{1}{\delta}}$$
\paragraph{} Executamos $\mathcal{A}$' $k$ vezes. Seja $z$ o número de vezes que $\mathcal{A}'(\sigma)=0$:
\begin{enumerate}
    \centering
    \item $\mathcal{A}(\sigma) = 1$, se $z \leq \frac{k}{e}$
    \item $\mathcal{A}(\sigma) = 0$, se $z > \frac{k}{e}$
\end{enumerate}
\subsection{Análise de $\mathcal{A}$}
\paragraph{} Suponha $\Delta \geq (1+\epsilon)T$, então:
$$E(z) = \mathcal{P}_0 k \leq \frac{1}{e}(1-\frac{\epsilon}{2})k$$
\paragraph{} Temos uma resposta errada se $z > \frac{k}{e}$
$$\frac{k}{e} = \frac{k}{e}(1-\frac{\epsilon}{2}) + \frac{k\epsilon}{2e} \geq E(z) + \frac{k\epsilon}{2e}$$
\paragraph{} Assim, utilizando McDiarmid:
$$\mathcal{P}(z>\frac{k}{e}) \leq \mathcal{P}(z > E(Z)+\frac{k\epsilon}{2e}) \leq exp(-\frac{\epsilon^2k}{2e^2})\leq \delta$$

\paragraph{} Suponha $\Delta \leq (1-\epsilon)T$, então:
$$E(z) = \mathcal{P}_0 k \geq \frac{1}{e}(1+\frac{\epsilon}{2})k$$
\paragraph{} Temos uma resposta errada se $z \leq \frac{k}{e}$
$$\frac{k}{e} = \frac{k}{e}(1+\frac{\epsilon}{2}) - \frac{k\epsilon}{2e} \geq E(z) - \frac{k\epsilon}{2e}$$
\paragraph{} Assim, utilizando McDiarmid:
$$\mathcal{P}(z \leq \frac{k}{e}) \leq \mathcal{P}(z \leq E(Z) - \frac{k\epsilon}{2e}) \leq \delta$$

\paragraph{} Podemos rodar o algoritmo A para os seguintes casos, com
$T_0=\frac{2}{\epsilon}$. Supunha $s \approx T=T_0(1+2\epsilon)^l$. Então:
$$s \leq T_0(1+2\epsilon)^l=\frac{1}{1+2\epsilon}(1+2\epsilon)^{l+1}T_0\leq (1-\epsilon)(T_0 ...)=0$$
\paragraph{}no caso $T:l+1$

\paragraph{} Temos que a saída é o primeiro $T$ tal que $\mathcal{A}(\sigma)=0$ $\Rightarrow$ $\Delta=(1 \pm 5\epsilon)T$. 

\subsection{Desigualdade de Chernoff}
\paragraph{} Sejam $X_1,...,X_n$ variáveis aleatórias independentes com $\mathcal{P}(X_i = 1) = p$ e seja:
$$Y = \sum_{i=1}^n X_i$$ 
\paragraph{} Para todo $t\geq 0$, temos:
\begin{enumerate}
    \centering
    \item $\mathcal{P}(Y \leq E(Y)-t) \leq exp(-\frac{2t^t}{n})$
    \item $\mathcal{P}(Y \geq E(Y)+t) \leq exp(-\frac{2t^t}{n})$
\end{enumerate}
\paragraph{} McDiarmid generaliza o Chernoff acima.

\subsection{Algoritmo $\mathcal{B}$}
\paragraph{} Esse é o algoritmo final. Primeiro, executamos $\mathcal{A}$ com $T=T_0,...$.
\paragraph{} vimos que $\Delta=(1 \pm 5\epsilon)T$ com probabilidade $\mathcal{P} \leq L\delta=\eta$. Precisamos usar $\delta=\frac{\eta}{L}\geq\frac{\eta \epsilon}{\log{n}}$
$$\log{\frac{1}{\delta}}=\log{\log{n}}+\log{\frac{2}{\epsilon}}+\log{\frac{1}{\eta}}$$

\subsection{Implementação}
\paragraph{}Em vez de armazenar $S$, podemos usar uma função de hashing $h:[n] \rightarrow [j]$ e definir:
$$S = h^{-1}(1) ={ i \in [n], h(i) = 1}$$

