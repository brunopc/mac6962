\section{Aula 26 de Agosto de 2019}
\label{2019_08_26}


\subsection{SVD - Descomposição em Valores Singulares}
\[\]
Descomposição em Valores Singulares, ou SVD, é um tipo de fatorização de matrizes. Temos por ela que qualquer matriz $ A \in {\rm I\!R^{n\times d}}$ pode ser decomposta na forma $A = UDV^T$, em que:

\begin{itemize}
\item[$\ast$]$UU^T = U^TU = I, U \in \rm I\!R^{n\times n}$, ou seja, $U$ é uma matriz unitária
\item[$\ast$]$VV^T = V^TV = I, V \in \rm I\!R^{d\times d}$, ou seja, $V$ é uma matriz unitária
\item[$\ast$]$D \in \rm I\!R^{n\times d}$ é uma matriz diagonal tal que, sendo $\sigma_i$ o valor da soma da coluna $i$ de D.
 \[\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_r > 0 = \sigma_{r+1} = ... = \sigma_p \]
 
Com $ r = \posto(A)$ e $p = \min(n,d)$
\end{itemize}
\[\]
Chamaremos os $\sigma$s acima definidos de valores singulares da matriz A e estabeleceremos adicionalmente as seguintes notações:

\[U = [ u_1 | ... | u_n ]\]
\[V = [v_1 | ... | v_d]\]
\[\]
Dessa forma, a SVD também pode ser escrita como:
\[A = \sum_{1 \leq i \leq p} \sigma_i u_i v_i^T =  \sum_{1 \leq i \leq r} \sigma_i u_i v_i^T \]

\subsection{Normas para matrizes}

Denotaremos uma matriz $M$ de tamanho $n\times m$ por $\rm{(m_{ij})_{n\times d}}$.
Definimos então a norma Frobenius, denotada por norma $F$ de $M$ ou
$||M||_F$ como:
\[||M||_F = ( \sum_{i,j} m_{ij}^2)^{1/2} \]
Iremos também definir a norma 2, ou espectral, de $M$ denotada também por $||M||_2$ como $ \max_{1\leq i \leq n} \ \sigma_i = \sigma_1 $ na SVD de M.

\[\]
Vale também pontuar uma importante relação entre as normas Frobenius e 2 de uma matriz, essa sendo que para toda matriz $A$, $||M||_2 \leq ||M||_F$.

\subsection{Teorema de Eckard, Young e Minsky}
\[\]

\begin{teorema}
Seja $ A \in {\rm I\!R^{n\times d}}$ uma matriz e $A = UDV^T$ sua decomposição em valores singulares.\\

Tome $A_k = \sum_{1\leq i \leq k} \sigma_i u_i v_i^T $ para algum $k\leq \posto(A)$.\\

O Teorema de Eckard, Young e Minsky afirma que, para toda matriz $B$ tal que $\posto(B) \leq k$, $ ||A-A_k||_F \leq ||A-B||_F $.
\end{teorema}.

\begin{teorema}
Seja $ A \in {\rm I\!R^{n\times d}}$ uma matriz e $A = UDV^T$ sua decomposição em valores singulares.\\

Tome $A_k = \sum_{1\leq i \leq k} \sigma_i u_i v_i^T $ para algum $k\leq \posto(A)$.\\

O Teorema de Eckard, Young e Minsky afirma que, para toda matriz $B$ tal que $\posto(B) \leq k$, $ ||A-A_k||_2 \leq ||A-B||_2 $.
\end{teorema}.


Ou seja, tanto na norma Frobenius quanto na norma 2, a norma de $A-A_k$ é mínima entre as matrizes da forma $A-B$ com $\posto(B) \leq k$.

\subsection{Traços}

Seja $A$ uma matriz quadrada, iremos definir o traço de $A$, ou $Tr(A)$, como $\sum_{i=1}^n a_{ii}$.
\[\]
Algumas propriedades de traços de matriz são:

\begin{enumerate}
\item Seja $A \in   {\rm I\!R^{n\times d}}, B \in   {\rm I\!R^{d\times n}} $, $Tr(AB)=Tr(BA)$. Isso vale pois:
\begin{proof}
\[Tr(AB) =  \sum_{1 \leq i \leq n} \sum_{1\leq j \leq d} a_{ij}b_{ji} = \sum_{i,j} b_{ji} a_{ij} = Tr(BA)\]
\end{proof}

\item Seja $A \in  {\rm I\!R^{n\times d}} $, $ Tr(AA^T)= ||A||_F^2$. Isso vale pois:
\begin{proof}
\[ Tr(AA^T) = Tr((\sum_{1\leq k \leq d} a_{ik} a_{jk} )) = \sum_{1 \leq i \leq n} \sum_{1\leq k \leq d} a_{ik}^2 = ||A||_F^2 \]
\end{proof}

\item Seja $A \in  {\rm I\!R^{n\times d}} $, $ Tr(AA^T)= \sum_{1\leq i \leq r} \sigma_i^2$, com $r = \posto(A)$. Isso vale pois:
\begin{proof}
\[A = UDV^T \implies A^T = VD^T U^T \implies\]
\[AA^T = UDV^T VD^T U^T = UDD^T U^T\]
Portanto:
\[Tr(AA^T) = Tr(UDD^T U^T) = Tr(U^T UDD^T) = Tr(diag(\sigma_1^2, \sigma_2^2 , ...))\]
\[= \sum_{1\leq i \leq p} \sigma_i^2 = \sum_{1\leq i \leq r} \sigma_i^2, \ com \ r = \posto(A) \]\\
\end{proof}
\end{enumerate}

\subsection{Aproximações de baixo posto}

Iremos chamar de $V_ =  <v_1,...,v_k>$ o subespaço de posto $k$, com $1\leq k \leq r = \posto(A)$, que melhor aproxima os vetores linha $a_1, ... , a_n$ que constituem a matriz A.\\

$V_k$ [é o subespaço de posto $k$ que minimiza a soma das distâncias ao quadrado entre os vetores linha de $A$ e $V_k$. Ou seja:

\[\sum_{1\leq i \leq n} \dist^2(a_i,V_k) = \min_{dim(Z)=k} \sum_{1\leq i \leq n} \dist^2(a_i,Z) = \min_{dim(Z)\leq k} \sum_{1\leq i \leq n} \dist^2(a_i,Z)  \]\\

Toma $a \in {\rm I\!R^d}$ um vetor linha, $a = a_1,...,a_n$, e $V_k$ como definido anteriormente, seja:

\[ P_{V_k}(a_i) = \sum_{1\leq j \leq k}(a_i v_j)v_j^T  \]\\

Vamos agora definir $A_k$ como $\sum_{1\leq i\leq k}\sigma_i u_i v_i^T$. É fato que:

\begin{center}
$A_k = $ 
\begin{bmatrix}
a_1^{(k)}\\ ...\\a_n^{(k)}
\end{bmatrix} = 
\begin{bmatrix}
P_{V_k}(a_1)\\ ...\\P_{V_k}(a_n)
\end{bmatrix}
\end{center}

Usando o fato acima, temos:\\

\[\sum_{1\leq i\leq n} \dist^2(a_i,V_k) = \sum_{1\leq i\leq n} ||a_i = P_{V_k}(a_i)||^2 = ||A-A_k||_F^2\]

\subsection{Prova do teorema de Eckard, Young e Minsky (1)}

Queremos provar que a melhor aproximação em dimensão k para uma matriz A na norma Frobenius é dada por $A_k = \sum_{1\leq i \leq k} \sigma_i u_i v_i^T $.\\

Primeiramente note que:

\[||A-A_k||^2_F =|| \sum_{k+1\leq i \leq n} \sigma_i u_i v_i^T   \ ||^2_F =\sum_{k+1\leq i \leq n} \sigma_i^2 \]

Pela desigualdade triangular na norma espectral, se $A = A'+A''$ então $\sigma_i(A)\leq \sigma_i(A') +\sigma_i(A'')$.\\

Seja $A'_k$ e $A''_k$ as aproximações de posto $k$ de $A'$ e $A''$ como descrito acima. Temos então que para todo $i,j\geq 1$:
\[\sigma_i(A') +\sigma_j(A'') =  \sigma_i(A'- A'_{i-1}) +\sigma_j(A''-A''_{j-1}) \]
\[\geq \sigma_1 (A-A'_{i-1}-A''_{j-1})\geq \sigma_1 (A-A_{i+j-2}) = \sigma_{i+j-1}(A)   \]\\
Note que quando $A'=A-B_k$ e $A''=B_k$, $\sigma_{k+1}(B_k)=0$. Dessa forma podemos concluir que para $i\geq1$ e $j=k+1$, $\sigma_i(A-B_k)\geq \sigma_{k+i}(A)$.\\

Portanto, 
\[||A-B_k||^2_F =\sum_{1\leq i \leq n} \sigma_i (A-B_k)^2 \geq 
 \sum_{1\leq i \leq n} \sigma_i (A)^2 \geq||A-A_k||^2_F  \]

\begin{flushright}
$\square$
\end{flushright}

\subsection{Prova do teorema de Eckard, Young e Minsky (2)}

Queremos provar que a melhor aproximação em dimensão k para uma matriz A na norma 2 é dada por $A_k = \sum_{1\leq i \leq k} \sigma_i u_u v_i^T $.\\


Primeiramente note que:

\[||A-A_k||_2 =|| \sum_{k+1\leq i \leq n} \sigma_i u_i v_i^T   \ ||_2 =\sigma_{k+1} \]

Precisamos então mostrar que sendo $B_k = XY^T$ onde $X$ e $Y$ tem $k$ colunas, então $||A-A_k||_2=\sigma_{k+1}\leq ||A-B_k||_2$.

Como $Y$ tem $k$ colunas, deve haver uma combinação linear das primeiras $k+1$ colunas de $V$, ou seja,

\[w=\gamma_1v_1+...+\gamma_{k+1}v_{k+1}    \],

tal que $Y^Tw=0$. Tome um $w$ tal que $||w||_2=1$, ou seja, $\gamma_1^2+..+\gamma_{k+1}^2=1$. Então:

\[||A-B_k||_2\geq||(A-B_k)w||_2=||A||_2 = (\gamma_1^2\sigma_1^2+...+\gamma_{k+1}^2\sigma_{k+1}^2)^{1/2} \geq \sigma_{k+1}^2     \]

\begin{flushright}
$\square$
\end{flushright}



