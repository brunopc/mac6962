
\section{Aula 15 de Agosto de 2019}
\label{2019_08_15}

\subsection{Elementos de aprendizado de máquina}
Ao longo desse tópico considere:
\begin{enumerate}
    \item $\chi$: espaço de instâncias;
    \item $D$: distribuição de probabilidade de $\chi$;
    \item $S \subset \chi$: conjunto de treinamento amostrado, sendo que $S$ será construído de acordo com a distribuição de probabilidade de $\chi$
    \item $C^*$: conceito objetivo. A depender do contexto, $C^* \subset \chi$ ou $C^*: \chi \rightarrow \{-1,1\}$;
    \item $h$: hipótese. Também a depender do contexto, $h \subset \chi$ ou $h: \chi \rightarrow \{-1,1\}$, tal que, em algum sentido $h$ é próximo de $C^*$.
\end{enumerate}{}

\begin{exemple}
Se $\chi = \mathbb{R}$, então $\mathcal{H} = \{[a,b],\;a\leq b\}$.
\end{exemple}{}

\begin{exemple}
Se $\chi = \mathbb{R}^d$,  e $\forall \; \omega \in \mathbb{R}^d$, $\omega_0 \in \mathbb{R}$ tem-se $h_{\omega, \omega_0} = \{x \in \mathbb{R}^d : \left< x, \omega\right> \geq \omega_0\}$.
\end{exemple}{}

O sentido de proximidade de $h$ em relação a $C^*$, citado anteriormente, pode ser definido com maior exatidão da seguinte forma.
\begin{definition}
A função $err_D: \{-1,1\}  \rightarrow [0,1]$, tal que $err_D(h) = \mathbb{P}[h \bigtriangleup C^*]$ é chamada de erro real de $h$
\end{definition}{}

\begin{definition}
A função $err_S: \{-1,1\}  \rightarrow [0,1]$, tal que $err_S(h) = | S \cap (h \bigtriangleup C^*)| / |S|$ é chamada de erro de treinamento ou erro empírico.
\end{definition}{}

\begin{definition}
É chamado de \emph{Overfitting} a ocorrência $err_S(h) = 0$ ou pequeno e $err_D(h)$ grande.
\end{definition}{}

Para evitar \emph{Overfitting}, pode-se restringir a análise a uma determinada classe de hipóteses ou conceitos $$\mathcal{H}$$, tal que $\mathcal{H} \subset 2^\chi$.

\begin{exemple}
Considere um caso particular do exemplo anterior, com $\chi = \{-1,1\}^2$, com $\mathcal{H}$ restrito a $\chi$.
\end{exemple}{}

\begin{definition}
$h \in \mathcal{H}$ é dito $(S, \epsilon)\text{-ruim}$ se $err_S(h) = 0$ porém $err_D(h) \geq \epsilon$
\end{definition}{}

\begin{theorem}
Suponha $|\mathcal{H}| < \infty$. Sejam dados  $\epsilon, \delta > 0$, se $n = |S|$ é tal que
\begin{equation*}
n \geq \frac{1}{\epsilon}\left(\log{|\mathcal{H}|} + \log{\frac{1}{\delta}}\right)
\end{equation*}{}
então
\begin{equation*}
    \mathbb{P} \{\exists \; h \in \mathcal{H} \; \text{que é} \; (S, \epsilon)\text{-ruim}\} \leq \delta
\end{equation*}{}
\end{theorem}

\begin{proof}
Fixe $h \in \mathcal{H}$ tal que $err_D(h) > \epsilon$, então:
\begin{align*}
    \mathbb{P}\{h \; \text{ser} \; (S, \epsilon)\text{-ruim}\} &= \left(1 - \mathbb{P}\{h \triangle c^*\}\right)^n \\
    &=(1 - err_D(h))^n \leq (1 - \epsilon)^n \leq \exp(-n\epsilon)
\end{align*}
Deste modo,
\begin{align*}
\mathbb{P}\{\exists \; h \in \mathcal{H}; \; (S, \epsilon)\text{-ruim}\} &\leq \sum{\mathbb{P}\{h \; \text{ser} \; (S, \epsilon)\text{-ruim}\}} \\
&\leq |\mathcal{H}|\exp(-n\epsilon) \leq \delta
\end{align*}
\end{proof}{}

De acordo com esse resultado, se $|S|$ for suficientemente grande, então pode-se aprender aproximadamente com alta probabilidade.
Agora queremos verificar uma condição de convergência uniforme do erro, ou seja, queremos que $\forall \; h \in \mathcal{H}, \; |err_S(h) - err_D(h)| \leq \epsilon$.

\begin{definition}
Seja E um evento, então a variável aleatória
\[
   \mathbb{I} = 
  \begin{cases} 
   1 & \text{se } A \\
   0       & \text{cc }
  \end{cases}
\]
é chamada varável indicadora do evento $A$
\end{definition}

\begin{theorem}
\label{theorem_ap_mq_1}
Suponha $|\mathcal{H}| < \infty$. Sejam dados  $\epsilon, \delta > 0$, se $n = |S|$ é tal que
\begin{equation*}
n \geq \frac{1}{2\epsilon^2}\left(\log{|\mathcal{H}|} + \log{\frac{1}{\delta}}\right)
\end{equation*}{}
então
\begin{equation*}
    \mathbb{P} \{\exists \; h \in \mathcal{H} :|err_S(h) - err_D(h)| \leq \epsilon\} \leq \delta
\end{equation*}{}
\end{theorem}
Antes de realizarmos a demonstração, considerando um dado $h \in \mathcal{H}$ e $S = \{s_1, \dots, s_n\}$, façamos as seguintes observações, 
\begin{enumerate}
    \item se $X_j = \mathbb{I}\{h(s_j) \neq c^*(s_j)\}$, então $\mathbb{E}[X_j] = \mathbb{P}\{X_j = 1\} = err_D(d)$;
    \item se $Y = \sum_{1 \leq j \leq n}{X_j}$, então $Y = |S|err_S(h) = n*err_S(h)$
    \item $\mathbb{E}[Y] = \mathbb{E}[\sum_{1 \leq 1 \leq n}{X_i}] = n * err_D(h) \Rightarrow Y - \mathbb{E}[Y] = n(err_S(h) - err_D(h))$
\end{enumerate}{}
\begin{proof}
Sejam $h \in \mathcal{H}$, $S = \{s_1, \dots, s_n\}$, $X_j = \mathbb{I}\{h(s_j) \neq c^*(s_j)\}$ e $Y = \sum_{1 \leq j \leq n}{X_j}$, então, por Chernoff tem-se que:
\begin{align*}
    \mathbb{P}\{|Y - \mathbb{E}[Y]| \geq t\} = \mathbb{P}\{n|err_S(h) - err_D(h)| \geq n\epsilon\} &\leq 2\exp\{-2\frac{(n\epsilon)^2}{n}\} \\
    &= 2\exp\{2n\epsilon^2\}
\end{align*}
Desse modo,
\begin{align*}
    \mathbb{P}\{\exists h \in \mathcal{H} : |err_S(h) - err_D(h)| \geq \epsilon\} & \leq \\
    &\leq \sum_{h \in \mathcal{H}}\mathbb{P}\{|err_S(h) - err_D(h)| \geq \epsilon\} \leq \\
    &\leq 2|\mathcal{H}|\exp\left\{-2\frac{(n\epsilon)^2}{n}\right\} \leq \delta
\end{align*}{}

\end{proof}
\subsubsection{Exemplo - Aprendizado de Disjunções}
\begin{enumerate}
    \item $\chi = \{0,1\}^d$, por exemplo, $d$ \emph{features};
    \item $D$: distribuição de probabilidade de $\chi$;
    \item $C^*$: disjunção desses \emph{features}
    \item $\mathcal{H} = \{\text{disjunções dos features}\}$.
\end{enumerate}{}

Temos que $|\mathcal{H}| = 2^d$. Pelo Teorema \ref{theorem_ap_mq_1}, basta ter:
\begin{equation}
\label{eq_ap_mq_estrela}
    |S| \geq \frac{1}{\epsilon}\left(\log{|\mathcal{H}|} + \log{\frac{1}{\delta}}\right) = \left(d\log{2} + \log{\frac{1}{\delta}}\right)
\end{equation}

\subsubsection*{Algoritmo Elementar para Aprender disjunções}

Algorítimo:
\begin{enumerate}
    \item sejam $S = \{s^{(1)}\, \dots, s^{(x)}\}$ e $\mathbb{F} = \{1, \dots, d\}$;
    \item para cada $s^{(j)}$ com $c^*(s^{(j)}) = 1$ (Instância negativa) e para cada $i \in [d]$ tal que $X_i = 1$, faça $F \leftarrow F \\ \{i\}$;
    \item resolva a conjunção $h = \bigvee_{i \in F}X_i$.
\end{enumerate}

\begin{lemma*}
Se $c^*$ é uma disjunção então $h = A(S)$ é consistente com $c^*$ em $S$, ou seja, $h(s) = c^*(S)$
\end{lemma*}

\begin{proof}
Suponha que $X_i \subset c^*$, então $i \in F$ ao final do algoritmo.
Toda variável $X_i$ que ocorre em $c^*$, ocorre em $h = A(S)$, e além disso, tem-se que $c^*(S) = 1 \Rightarrow h(X) = 1$.

Suponha que $c^*(X) = -1$ para algum $s \in S$, então para todo $i \in [i]$, tem-se que $X_{i}^{\circ} = 1$, $x_i$ não ocorre em $h$. Desse modo $h(s^\circ) = -1$
\end{proof}

Pelo Teorema \ref{theorem_ap_mq_1}, deduzimos que, se vale a Equação \ref{eq_ap_mq_estrela}, então $err_D(h) \leq \epsilon$ com probabilidade maior que $1-\delta$ para $h=A(S)$.
