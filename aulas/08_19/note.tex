%region recapitulating
\section{Aula 19 de Agosto de 2019}
\label{2019_08_19}

\subsection*{\it Relembrando}

\paragraph{\nopunct} Sejam
\begin{enumerate}
  \item $\mathcal{X}$ espaço de instâncias;
  \item $\mathcal{D}\colon\ \mathcal{X}\mapsto\mathds{R}$ distribuição sobre $\mathcal{X}$;
  \item $\mathrm{S}\subseteq\mathcal{X}$ conjunto de treinamento, 
  $\mathrm{S}=\left\{s_1,\ \ldots,\ s_n\vphantom{1j}\right\},\ s_i\sim\mathcal{D}$ i.i.d.;
  \item $c^\star\subseteq\mathcal{X}$ conceito objetivo;
  \item $h\subseteq\mathcal{X}$ hipótese sobre $\mathcal{X}$;
  \item $\mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right)=\mathrm{P}\left(h\mathrel{\Delta}c^\star\vphantom{1j}\right)$, onde 
  $h\mathrel{\Delta}c^\star\vcentcolon=\left(h\setminus c^\star\vphantom{1j}\right)\cup\left(c^\star\setminus h\vphantom{1j}\right)$;
  \item $\mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right)=
  \nicefrac{\left|\left(h\mathrel{\Delta}c^\star\vphantom{1j}\right)\cap\mathrm{S}\vphantom{1j}\right|}{\left|\mathrm{S}\vphantom{1j}\right|}$; e
  \item $\mathcal{H}\subseteq 2^\mathcal{X}$, com $h\in\mathcal{H}$ \textbf{classe de hipóteses} 
  (ou classe de conceitos).
\end{enumerate}

Temos

\textbf{Teorema 1 (ref:label?):} $\eps,\ \delta>0$. Se
\[
  n = \left|\mathrm{S}\vphantom{1j}\right| \geq \frac{1}{\eps}\cdot\left(\log\left|\mathcal{H}\vphantom{1j}\right| + \log\nicefrac{1}{\delta}\vphantom{1j}\right),
\]

então
\[
  \mathrm{P}\left(\exists h\in\mathcal{H}\colon\ \left(\mathrm{S},\ \eps\vphantom{1j}\right)\text{-ruim}\vphantom{1j}\right) \leq \delta
\]

\textbf{Definição ? (ref:label?):} $h$ é \textbf{$\left(\mathrm{S},\ \eps\vphantom{1j}\right)$-ruim} se $\mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right)=0$, mas $\mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right)>\eps$.

\textbf{Teorema 2 (ref:label?):} $\eps,\ \delta>0$. Se
\[
  n = \left|\mathrm{S}\vphantom{1j}\right| \geq \frac{1}{2\cdot\eps^2}\cdot\left(\log\left|\mathcal{H}\vphantom{1j}\right| + \log\nicefrac{2}{\delta}\vphantom{1j}\right),
\]

então
\[
  \mathrm{P}\left(\left|\mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right)-\mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right)\vphantom{1j}\right| > \eps\vphantom{1j}\right) \leq \delta
\]
%endregion

%region occam's razor
\clearpage
\subsection{Navalha de Occam}

\paragraph{\nopunct}[\textit{Sugestão: <\href{https://en.wikipedia.org/wiki/Philosophical\_razor}{https://en.wikipedia.org/wiki/Philosophical\_razor}>}]

\begin{afirmacao}
  \label{afr:navalha_de_occam}
  \normalfont
  \textbf{Navalha de Occam:} damos preferência a conceitos/hipóteses simples.
\end{afirmacao}

\paragraph{\nopunct} Seja $\mathcal{L}$ linguagem para descrever as $h\in\mathcal{H}$.

\begin{exemplo}
  $\mathcal{H}=\left\{\text{disjunções}\vphantom{1j}\right\}$, e.g. $h\left(x_1,\ \ldots,\ x_d\vphantom{1j}\right)=x_2\lor x_5\lor x_7$
  
  segue que $\mathcal{L}=\left\{0,\ 1\vphantom{1j}\right\}^d$.
\end{exemplo}

\begin{exemplo}
  $\mathcal{H}=\left\{\text{funções booleanas}\vphantom{1j}\right\}$, $h\colon\ \left\{0,\ 1\vphantom{1j}\right\}^d\mapsto\left\{-1,\ 1\vphantom{1j}\right\}$
  
  segue que $\mathcal{L}=\left\{\text{árvores de decisão}\vphantom{1j}\right\}=\left\{\text{circuitos booleanos}\vphantom{1j}\right\}$.
\end{exemplo}

Dado $\mathcal{L}$, podemos definir $\mathrm{size}\left(h\vphantom{1j}\right)=\#\mathrm{bits}$ para especificar $h$ na linguagem $\mathcal{L}$.

Vamos considerar
\[
  \mathcal{L}_b = \left\{h\in\mathcal{H}\colon\ \mathrm{size}\left(h\vphantom{1j}\right)=b\vphantom{1j}\right\}
\]

e
\[
  \mathcal{L}_{<b} = \bigcup_{b'<b} \mathcal{L}_b.
\]

\begin{observacao}
  $\left|\mathcal{L}_b\vphantom{1j}\right| \leq 2^b$
  \[
    \left|\mathcal{L}_{<b}\vphantom{1j}\right| \leq 1 + 2 + \ldots + 2^{b-1} < 2^b
  \]
\end{observacao}

\begin{teorema}
  \label{teo:teo4}
  \normalfont
  (teorema 4...)
  (Cor do Teo1 (ref:label?))
  Fixe $\eps,\ \delta>0$ e suponha
  \[
    n = \left|\mathrm{S}\vphantom{1j}\right| \geq \frac{1}{\eps}\cdot\left(b\cdot\log2 + \log\nicefrac{1}{\delta}\vphantom{1j}\right).
  \]

  Então
  \[
    \mathrm{P}\left(\exists h\in\mathcal{H}\colon\ \left(\mathrm{S},\ \eps\vphantom{1j}\right)\text{-ruim}\vphantom{1j}\right) \leq \delta
  \]
\end{teorema}

Equivalentemente, com probabilidade $\geq 1-\delta$,

se $\mathrm{err}_\mathcal{S}\left(h\vphantom{1j}\right)$ e $h\in\mathcal{L}_{<b}$,

então
\[
  \mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right) \leq \frac{b\cdot\log2 + \log\nicefrac{1}{\delta}}{\left|\mathrm{S}\vphantom{1j}\right|}.
\]

\begin{teorema}
  \label{teo:teo5}
  \normalfont
  (teorema 5...)
  (Cor do Teo2 (ref:label?))
  Fixe $\eps,\ \delta>0$ e suponha
  \[
    n = \left|\mathrm{S}\vphantom{1j}\right| \geq \frac{1}{2\cdot\eps^2}\cdot\left(b\cdot\log2 + \log\nicefrac{2}{\delta}\vphantom{1j}\right).
  \]

  Então, com probabilidade $\geq 1-\delta$, temos que se $h\in\mathcal{L}_{<b}$, então
  \[
    \mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right) \leq \mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right) + \eps
  \]
\end{teorema}

\begin{corolario}
  \normalfont
  Equivalente: $h\in\mathcal{L}_{<b}$, $\delta>0$, com probabilidade $\geq 1-\delta$,
  Fixe $\eps,\ \delta>0$ e suponha
  \[
    \mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right) \leq \mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right) + \sqrt{\frac{1}{2\cdot\left|\mathrm{S}\vphantom{1j}\right|}\cdot\left(b\cdot\log2 + \log\nicefrac{2}{\delta}\vphantom{1j}\right)}.
  \]
\end{corolario}

Nos Teoremas \autoref{teo:teo4} e \autoref{teo:teo5}, $b$ é fixo. Se quisermos considerar vários valores de $b$ simultaneamente, podemos fazer o sequinte.

Fixe $\delta>0$. Seja $\delta_i=\nicefrac{\delta}{2^i},\ i=1,\ 2,\ \ldots$. Então $\sum_1^\infty \delta_i=\delta$.

Consideramos $\mathcal{L}_b$ para $b=0,\ 1,\ 2,\ \ldots$. Aplicando o Teorema 2 (ref:label?) para $\mathcal{L}_b$ e $\delta_i$, temos que, com prob. $\geq 1-\delta_i$, se $h\in\mathcal{L}_b$ então
\begin{align*}
  \mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right) &\leq \mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right) + \sqrt{\frac{1}{2\cdot\left|\mathrm{S}\vphantom{1j}\right|}\cdot\left(i\cdot\log2 + \log\nicefrac{2}{\delta_i}\vphantom{1j}\right)}. \\
                                  &= \mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right) + \sqrt{\frac{1}{2\cdot\left|\mathrm{S}\vphantom{1j}\right|}\cdot\left(i\cdot\log4 + \log\nicefrac{2}{8}\vphantom{1j}\right)}.
\end{align*}

Concluímos que, com prob. $\geq 1-\delta$,
\[
  \mathrm{err}_\mathcal{D}\left(h\vphantom{1j}\right) \leq = \mathrm{err}_\mathrm{S}\left(h\vphantom{1j}\right) + \sqrt{\frac{1}{2\cdot\left|\mathrm{S}\vphantom{1j}\right|}\cdot\left(\mathrm{size}\left(h\vphantom{1j}\right)\cdot\log4 + \log\nicefrac{2}{8}\vphantom{1j}\right)}.
\]

Podemos agora minimizar o todo direto variando $h\in\mathcal{H}$
%endregion

%region decision trees
\clearpage
\subsection{Árvores de decisão}

\paragraph{\nopunct}[\textit{Sugestão: <\href{https://scikit-learn.org/stable/modules/tree.html}{https://scikit-learn.org/stable/modules/tree.html}>}]

\begin{exemplo}
  \begin{tikzpicture}[
    auto, thick,
    node/.style={draw, circle, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    star/.style={draw, diamond, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    block/.style={draw, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    title/.style={draw=none, circle, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    every loop/.style={}
    ]
    \node[node]   (x2)                                        {$x_2$};
    \node[node]   (x1)  [below=0.75 of x2, xshift=-2.00cm]    {$x_1$};
    \node[node]   (x3)  [below=0.75 of x2, xshift= 2.00cm]    {$x_3$};
    \node[node]   (x4)  [below=0.75 of x3, xshift= 1.00cm]    {$x_1$};
    
    \node[block]  (y1)  [below=0.75 of x1, xshift=-1.00cm]    {$+$};
    \node[block]  (y2)  [below=0.75 of x1, xshift= 1.00cm]    {$-$};
    \node[block]  (y3)  [below=0.75 of x3, xshift=-1.00cm]    {$+$};
    \node[block]  (y4)  [below=0.75 of x4, xshift=-1.00cm]    {$-$};
    \node[block]  (y5)  [below=0.75 of x4, xshift= 1.00cm]    {$+$};
    
    \path[->]
    (x2)  edge  node[above left]  {$0$}  (x1)
    (x2)  edge  node[above right] {$1$}  (x3)
    (x1)  edge  node[above left]  {$0$}  (y1)
    (x1)  edge  node[above right] {$1$}  (y2)
    (x3)  edge  node[above left]  {$0$}  (y3)
    (x3)  edge  node[above right] {$1$}  (x4)
    (x4)  edge  node[above left]  {$0$}  (y4)
    (x4)  edge  node[above right] {$1$}  (y5)
    ;
  \end{tikzpicture}

  Fórmula booleana equivalente: $\overline{x_1}\overline{x_2}\lor x_2\overline{x_1}\lor x_1x_2x_3$.

  {
    \delimitershortfall=-1pt                        % bigger wrapping brackets
    $\mathcal{L}$: 
    $\left(x_2
            \left(x_1
                  \left(+\vphantom{1j}\right)
                  \left(-\vphantom{1j}\right)
            \vphantom{1j}\right)
            \left(x_3
                  \left(+\vphantom{1j}\right)
                  \left(x_1
                      \left(-\vphantom{1j}\right)
                      \left(+\vphantom{1j}\right)
                  \vphantom{1j}\right)
            \vphantom{1j}\right)
      \vphantom{1j}\right)$
  }

  Alfabeto: $\left\{1,\ \ldots,\ d\vphantom{1j}\right\}\cup\left\{\text{'('},\ \text{')'},\ \text{'+'},\ \text{'-'}\vphantom{1j}\right\}$, $\left\lceil\lg\left(d+4\vphantom{1j}\right)\vphantom{1j}\right\rceil$ bits por símbolo.
\end{exemplo}

\begin{exercicio}
  Seja $f(k)$ o número de símbolos que precisamos para representar uma árvore de decisão com $k$ nós externos em $\mathcal{L}$. Prove que
  \[
    f(k) = 3\cdot\left(2\cdot k-1\vphantom{1j}\right),\ k\geq 1
  \]

  Representação de uma árvore de decisão com $k$ nós externos:
  \[
    3\cdot\left(2\cdot k-1\vphantom{1j}\right)\cdot\left\lceil\lg\left(d+4\vphantom{1j}\right)\vphantom{1j}\right\rceil = \mathcal{O}\left(k\cdot\log d\vphantom{1j}\right) \text{bits}
  \]
\end{exercicio}
\begin{resolucao}
  Os $k$ nós externos contribuem com $3$ símbolos cada: '(+)' ou '(-)'.

  Os $k-1$ nós internos contribuem com $3$ símbolos cada: '(x' e ')'

  Resultado: $3\cdot k + 3\cdot\left(k-1\vphantom{1j}\right) = 3\cdot\left(2\cdot k-1\vphantom{1j}\right)$.

  O fator $\cdot\left\lceil\lg\left(d+4\vphantom{1j}\right)\vphantom{1j}\right\rceil$ é pela quantidade mínima de bits para representar o alfabeto.
\end{resolucao}

Dado $\mathrm{S}\subseteq\left\{0,\ 1\vphantom{1j}\right\}^d$, como obtemos uma Árvore de Decisão $T$ com $\mathrm{err}_\mathrm{S}\left(T\vphantom{1j}\right)=0$?

\begin{exercicio}
  Fácil obter uma $T$ com $\leq2\cdot\left|\mathrm{S}\vphantom{1j}\right|$ nós externos.
\end{exercicio}
\begin{resolucao}
  Uma árvore balanceada com $2^d=\left|\mathrm{S}\vphantom{1j}\right|$ nós externos é capaz de estilhaçar $\mathrm{S}\subseteq\left\{0,\ 1\vphantom{1j}\right\}^d$
\end{resolucao}

Assim
\[
  \mathrm{size}\left(T\vphantom{1j}\right) = \mathcal{O}\left(\left|\mathrm{S}\vphantom{1j}\right|\log d\vphantom{1j}\right).
\]

Se conseguimos $T$ com (pelo Teo 4 (ref:label?))
\[
  \mathrm{size}\left(T\vphantom{1j}\right) \leq \frac{\eps}{2\cdot\log 2} \left|\mathrm{S}\vphantom{1j}\right|
\]

e $\left|\mathrm{S}\vphantom{1j}\right|\geq\frac{2}{\eps}\cdot\log\nicefrac{1}{\delta}$, então com prob. $\geq 1-\delta$,
\begin{align}
  \mathrm{err}_\mathcal{D}\left(T\vphantom{1j}\right) &\leq \frac{\mathrm{size}\left(T\vphantom{1j}\right)\cdot\log2 + \log\nicefrac{1}{\delta}}{\left|\mathrm{S}\vphantom{1j}\right|}  \\
                                  &\leq \frac{\eps}{2} + \frac{\eps}{2} = \eps
\end{align}

Podemos deduzir algo análogo para o Teo. 5 (ref:label?).

\begin{fato}
  Obter uma menor $T$ tal que $\mathrm{err}_\mathrm{S}\left(T\vphantom{1j}\right)=0$ é NP-difícil.
\end{fato}
%endregion

%region Iterative dichotomizer
\clearpage
\subsection{Heurística ID3}

\paragraph{\nopunct}[\textit{Sugestão: <\href{https://en.wikipedia.org/wiki/Mutual\_information}{https://en.wikipedia.org/wiki/Mutual\_information}>}]

Considere a partição $\mathrm{S}=\mathrm{S}^+\cup\mathrm{S}^-$ de instâncias positivas e negativas.

Seja também a indicação $\mathrm{S}_{x_i=0}\vcentcolon=\left\{s\in \mathrm{S}\colon\ x_i=0\vphantom{1j}\right\}$

\begin{definicao}
  \normalfont
  \textbf{Entropia binária}
  \[
    \mathrm{H}\left(p\vphantom{1j}\right) = \mathrm{H}_2\left(p\vphantom{1j}\right) \vcentcolon= p\cdot\lg\nicefrac{1}{p} + q\cdot\lg\nicefrac{1}{q}
  \]

  onde $p+q=1$ e $0\leq 0\leq1$.

  Convenção: $0\cdot\log\nicefrac{1}{0}=0\cdot\log0=0$
\end{definicao}

\[
  \mathrm{H}\left(\mathrm{S}\vphantom{1j}\right) = \mathrm{H}\left(\nicefrac{\left|\mathrm{S}^+\vphantom{1j}\right|}{\left|\mathrm{S}\vphantom{1j}\right|}\vphantom{1j}\right)
\]

\[
  \mathrm{H}\left(\mathrm{S}\,\middle|\, x_i\vphantom{1j}\right) = \frac{\mathrm{S}_{x_i=0}}{\mathrm{S}}\cdot\mathrm{H}\left(\mathrm{S}_{x_i=0}\vphantom{1j}\right) + \frac{\mathrm{S}_{x_i=1}}{\mathrm{S}}\cdot\mathrm{H}\left(\mathrm{S}_{x_i=1}\vphantom{1j}\right)
\]

\begin{definicao}
  \normalfont
  Heurística: escolhemos $x_i$ para minimizar o ganho de informação.

  \textbf{Information Gain:}
  \[
    \mathrm{IG}\left(i\vphantom{1j}\right) \vcentcolon= \mathrm{H}\left(S\vphantom{1j}\right) - \mathrm{H}\left(\mathrm{S}\,\middle|\, x_i\vphantom{1j}\right).
  \]

  \begin{tikzpicture}[
    auto, thick,
    node/.style={draw, circle, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    star/.style={draw, diamond, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    block/.style={draw, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    title/.style={draw=none, circle, thick, text centered, 
    minimum height=0.50cm, minimum width=0.50cm},
    every loop/.style={}
    ]
    \node[node]   (x1)                                        {$x_1$};
    
    \node[block]  (y1)  [below=0.75 of x1, xshift=-1.00cm]    {$\mathrm{S}_{x_i=0}$};
    \node[block]  (y2)  [below=0.75 of x1, xshift= 1.00cm]    {$\mathrm{S}_{x_i=1}$};
    
    \path[->]
      (x1)  edge  node[above left]  {$0$}  (y1)
      (x1)  edge  node[above right] {$1$}  (y2)
    ;
  \end{tikzpicture}
\end{definicao}
%endregion


